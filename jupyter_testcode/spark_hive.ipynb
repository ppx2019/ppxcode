{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 9, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f538fd96e395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 9, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "keyconv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"\n",
    "valueconv = \"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\"\n",
    "conf = {\n",
    "    \"hbase.zookeeper.quorum\": \"cdh1:2181\",\n",
    "    \"hbase.mapreduce.inputtable\": \"new_energy_obd_info\",\n",
    "    \"hbase.mapreduce.scan.column.family\": \"A\",\n",
    "    #B:\"a01\"为16进制的原始数据\n",
    "    #\"hbase.mapreduce.scan.columns\": \"B:a01\",\n",
    "}\n",
    "#conf[\"hbase.mapreduce.scan.row.start\"] = \"LEWPCA100JF260246__2019-10-29 00:00:00\"\n",
    "#conf[\"hbase.mapreduce.scan.row.stop\"] = \"LEWPCA100JF260246__2019-10-29 23:59:59\" \n",
    "rdd = sc.newAPIHadoopRDD(\n",
    "    \"org.apache.hadoop.hbase.mapreduce.TableInputFormat\",\n",
    "    \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "    \"org.apache.hadoop.hbase.client.Result\",\n",
    "    keyConverter=keyconv,\n",
    "    valueConverter=valueconv,\n",
    "    conf=conf,\n",
    ")\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = {\n",
    "    'h04': 'motor_faults_count', \n",
    "    'h23': 'braking_system_alert', \n",
    "    'a02': 'obd_time', \n",
    "    'a03': 'receive_time',\n",
    "    'a01': 'vin',\n",
    "    'h06': 'engine_faults_count',\n",
    "    'i02': 'resd_subsystem_voltage_list',\n",
    "    'i03': 'resd_subsystem_temperature_list',\n",
    "    'i01': 'resd_subsystem_count',\n",
    "    'e03': 'engine_fuel_consumption_rate',\n",
    "    'h16': 'single_battery_low_voltage_alert',\n",
    "    'g07': 'maximum_temperature_subsystem_code',\n",
    "    'g08': 'maximum_temperature_probe_code',\n",
    "    'b13': 'insulation_resistance',\n",
    "    'h17': 'soc_too_high_alert',\n",
    "    'h21': 'insulation_alert',\n",
    "    'b01': 'car_status',\n",
    "    'h09': 'other_fault_codes',\n",
    "    'h08': 'other_faults_count',\n",
    "    'd10': 'hydrogen_maximum_pressure',\n",
    "    'h28': 'energy_storage_device_over_charge_alert',\n",
    "    'e02': 'crankshaft_speed',\n",
    "    'h25': 'motor_controller_temperature_alert',\n",
    "    'h24': 'dc_dc_status_alert',\n",
    "    'h01': 'highest_alert_level',\n",
    "    'h26': 'hvil_alert',\n",
    "    'h07': 'engine_fault_codes',\n",
    "    'h20': 'battery_cell_consistency_low_alert',\n",
    "    'h05': 'motor_fault_codes',\n",
    "    'h22': 'dc_dc_temperature_alert',\n",
    "    'f05': 'south_or_north',\n",
    "    'f04': 'is_location_valid',\n",
    "    'b14': 'accelerator',\n",
    "    'b15': 'brake',\n",
    "    'f01': 'lng',\n",
    "    'g09': 'maximum_temperature_value',\n",
    "    'b10': 'gear',\n",
    "    'f02': 'lat',\n",
    "    'g04': 'minimum_voltage_battery_subsystem_code',\n",
    "    'g05': 'minimum_voltage_battery_cell_code',\n",
    "    'g06': 'battery_cell_voltage_lowest_value',\n",
    "    'e01': 'engine_status',\n",
    "    'g01': 'maximum_voltage_battery_subsystem_code',\n",
    "    'g02': 'maximum_voltage_battery_cell_code',\n",
    "    'g03': 'battery_cell_voltage_highest_value',\n",
    "    'd07': 'hydrogen_system_highest_temperature_probe_code',\n",
    "    'f03': 'address',\n",
    "    'd06': 'hydrogen_system_highest_temperature',\n",
    "    'f06': 'east_or_west',\n",
    "    'b11': 'can_brake',\n",
    "    'h02': 'battery_faults_count',\n",
    "    'c01': 'motor_count',\n",
    "    'b12': 'can_drive',\n",
    "    'd11': 'hydrogen_maximum_pressure_sensor_code',\n",
    "    'h18': 'soc_jumping_alert',\n",
    "    'h19': 'rechargeable_energy_storage_system_mismatch_alert',\n",
    "    'd05': 'probe_temperature_list',\n",
    "    'd04': 'fuel_cell_temperature_probes_count',\n",
    "    'd03': 'cell_fuel_consumption_rate',\n",
    "    'd02': 'fuel_cell_current',\n",
    "    'd01': 'fuel_cell_voltage',\n",
    "    'd12': 'high_voltage_dc_dc_status',\n",
    "    'h10': 'temperature_difference_alert',\n",
    "    'h11': 'battery_high_temperature_alert',\n",
    "    'h12': 'energy_storage_device_high_voltage_alert',\n",
    "    'h13': 'energy_storage_device_low_voltage_alert',\n",
    "    'h14': 'soc_low_alert',\n",
    "    'h15': 'single_battery_high_voltage_alert',\n",
    "    'd09': 'hydrogen_highest_concentration_sensor_code',\n",
    "    'd08': 'hydrogen_highest_concentration',\n",
    "    'h27': 'motor_temperature_alert',\n",
    "    'h03': 'battery_fault_codes',\n",
    "    'b03': 'running_mode',\n",
    "    'b02': 'charge_status',\n",
    "    'b05': 'mileage',\n",
    "    'b04': 'speed',\n",
    "    'b07': 'current',\n",
    "    'b06': 'voltage',\n",
    "    'b09': 'dc_dc_status',\n",
    "    'b08': 'soc',\n",
    "    'c02': 'motor_info_list',\n",
    "    'g12': 'minimum_temperature_value',\n",
    "    'g11': 'minimum_temperature_probe_code',\n",
    "    'g10': 'minimum_temperature_subsystem_code'}\n",
    "chart_b = {\n",
    "    \"a01\": \"hex_data\",\n",
    "    \"a02\": \"obd_time\",\n",
    "    \"a03\": \"receive_time\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rdd.values().map(lambda row: dict(\n",
    "            (chart.get(json.loads(item).get(\"qualifier\")),  #获取指定键\n",
    "             json.loads(item).get(\"value\")  #获取指定值\n",
    "             .replace('\"', \"temp_data\")\n",
    "             .replace(\"'\", '\"')\n",
    "             .replace(\"temp_data\", \"'\")\n",
    "             ) for item in row.split(\"\\n\")))\n",
    "result.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将rdd转为DF格式并建立临时表\n",
    "#result.saveAsTextFile(\"/User/ppx\")\n",
    "#df=result.toDF().show(1)\n",
    "result.toDF().registerTempTable(\"new_energy_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"psark to hive\") \\\n",
    ".config(\"spark.sql.warehouse.dir\", \"hdfs://cdh1:8020/warehouse/tablespace/external/hive/dfjk.db/\") \\\n",
    ".config(\"hive.metastore.uris\", \"thrif://cdh2:9083\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"use dfjk\").show()\n",
    "#spark.sql(\"create table new_energy_obd_info(accelerator string,battery_cell_consistency_low_alert string,battery_cell_voltage_highest_value string,battery_cell_voltage_lowest_value string,battery_fault_codes string,battery_faults_count string,battery_high_temperature_alert string,brake string,braking_system_alert string,can_brake string,can_drive string,car_status string,cell_fuel_consumption_rate string,charge_status string,crankshaft_speed string,current string,dc_dc_status string,dc_dc_status_alert string,dc_dc_temperature_alert string,east_or_west string,energy_storage_device_high_voltage_alert string,energy_storage_device_low_voltage_alert string,energy_storage_device_over_charge_alert string,engine_fault_codes string,engine_faults_count string,engine_fuel_consumption_rate string,engine_status string,fuel_cell_current string,fuel_cell_temperature_probes_count string,fuel_cell_voltage string,gear string,high_voltage_dc_dc_status string,highest_alert_level string,hvil_alert string,hydrogen_highest_concentration string,hydrogen_highest_concentration_sensor_code string,hydrogen_maximum_pressure string,hydrogen_maximum_pressure_sensor_code string,hydrogen_system_highest_temperature string,hydrogen_system_highest_temperature_probe_code string,insulation_alert string,insulation_resistance string,is_location_valid string,lat string,lng string,maximum_temperature_probe_code string,maximum_temperature_subsystem_code string,maximum_temperature_value string,maximum_voltage_battery_cell_code string,maximum_voltage_battery_subsystem_code string,mileage string,minimum_temperature_probe_code string,minimum_temperature_subsystem_code string,minimum_temperature_value string,minimum_voltage_battery_cell_code string,minimum_voltage_battery_subsystem_code string,motor_controller_temperature_alert string,motor_count string,motor_fault_codes string,motor_faults_count string,motor_info_list string,motor_temperature_alert string,obd_time string,other_fault_codes string,other_faults_count string,probe_temperature_list string,receive_time string,rechargeable_energy_storage_system_mismatch_alert string,running_mode string,single_battery_high_voltage_alert string,single_battery_low_voltage_alert string,soc string,soc_jumping_alert string,soc_low_alert string,soc_too_high_alert string,south_or_north string,speed string,temperature_difference_alert string,vin string,voltage string)location '/warehouse/tablespace/external/hive/dfjk.db/' stored as parquet\") \n",
    "spark.sql(\"show tables\").show()\n",
    "#spark.sql(\"insert into new_energy_obd_info select * from new_energy_temp\")\n",
    "#spark.sql(\"alter table new_energy_obd_info add patition(receive_time)\")\n",
    "#spark.sql(\"alter table new_energy_obd_info add patition(receive_time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use dfjk\").show()\n",
    "spark.sql(\"show tables\").show()\n",
    "#spark.sql(\"drop table new_energy_obd_info\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19个报警字段\n",
    "df1=spark.sql(\"select temperature_difference_alert,battery_high_temperature_alert,energy_storage_device_high_voltage_alert,energy_storage_device_low_voltage_alert,soc_low_alert,single_battery_high_voltage_alert,single_battery_low_voltage_alert,soc_too_high_alert,soc_jumping_alert,rechargeable_energy_storage_system_mismatch_alert,battery_cell_consistency_low_alert,insulation_alert,dc_dc_temperature_alert,braking_system_alert,dc_dc_status_alert,motor_controller_temperature_alert,hvil_alert,motor_temperature_alert,energy_storage_device_over_charge_alert from new_energy_obd_info\")\n",
    "#统计数据条数\n",
    "spark.sql(\"select count(*) from new_energy_obd_info\").show()\n",
    "#df1.write.csv('/192.168.3.155:8020//Users/ppx/Desktop')\n",
    "#spark.sql(\"select * from new_energy_obd_info\").write.saveAsTable(\"new_energy_obd_info1\")\n",
    "#df1.save('hdfs://192.168.3.155:9000/Users/ppx/Desktop')\n",
    "df1.show(n=1)\n",
    "df1.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-f28813e169b0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-f28813e169b0>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    splited=df1.randomSplit(dict(0.7,0.3),2L)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import *\n",
    "from pyspark.mllib.classification import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "splited=df1.randomSplit(dict(0.7,0.3),2L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
