{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "keyconv = \"org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter\"\n",
    "valueconv = \"org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter\"\n",
    "conf = {\n",
    "    \"hbase.zookeeper.quorum\": \"cdh1:2181\",\n",
    "    \"hbase.mapreduce.inputtable\": \"new_energy_obd_info\",\n",
    "    \"hbase.mapreduce.scan.column.family\": \"A\",\n",
    "    #B:\"a01\"为16进制的原始数据\n",
    "    #\"hbase.mapreduce.scan.columns\": \"B:a01\",\n",
    "}\n",
    "#conf[\"hbase.mapreduce.scan.row.start\"] = \"LEWPCA100JF260246__2019-10-29 00:00:00\"\n",
    "#conf[\"hbase.mapreduce.scan.row.stop\"] = \"LEWPCA100JF260246__2019-10-29 23:59:59\" \n",
    "rdd = sc.newAPIHadoopRDD(\n",
    "    \"org.apache.hadoop.hbase.mapreduce.TableInputFormat\",\n",
    "    \"org.apache.hadoop.hbase.io.ImmutableBytesWritable\",\n",
    "    \"org.apache.hadoop.hbase.client.Result\",\n",
    "    keyConverter=keyconv,\n",
    "    valueConverter=valueconv,\n",
    "    conf=conf,\n",
    ")\n",
    "\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = {\n",
    "    'h04': 'motor_faults_count', \n",
    "    'h23': 'braking_system_alert', \n",
    "    'a02': 'obd_time', \n",
    "    'a03': 'receive_time',\n",
    "    'a01': 'vin',\n",
    "    'h06': 'engine_faults_count',\n",
    "    'i02': 'resd_subsystem_voltage_list',\n",
    "    'i03': 'resd_subsystem_temperature_list',\n",
    "    'i01': 'resd_subsystem_count',\n",
    "    'e03': 'engine_fuel_consumption_rate',\n",
    "    'h16': 'single_battery_low_voltage_alert',\n",
    "    'g07': 'maximum_temperature_subsystem_code',\n",
    "    'g08': 'maximum_temperature_probe_code',\n",
    "    'b13': 'insulation_resistance',\n",
    "    'h17': 'soc_too_high_alert',\n",
    "    'h21': 'insulation_alert',\n",
    "    'b01': 'car_status',\n",
    "    'h09': 'other_fault_codes',\n",
    "    'h08': 'other_faults_count',\n",
    "    'd10': 'hydrogen_maximum_pressure',\n",
    "    'h28': 'energy_storage_device_over_charge_alert',\n",
    "    'e02': 'crankshaft_speed',\n",
    "    'h25': 'motor_controller_temperature_alert',\n",
    "    'h24': 'dc_dc_status_alert',\n",
    "    'h01': 'highest_alert_level',\n",
    "    'h26': 'hvil_alert',\n",
    "    'h07': 'engine_fault_codes',\n",
    "    'h20': 'battery_cell_consistency_low_alert',\n",
    "    'h05': 'motor_fault_codes',\n",
    "    'h22': 'dc_dc_temperature_alert',\n",
    "    'f05': 'south_or_north',\n",
    "    'f04': 'is_location_valid',\n",
    "    'b14': 'accelerator',\n",
    "    'b15': 'brake',\n",
    "    'f01': 'lng',\n",
    "    'g09': 'maximum_temperature_value',\n",
    "    'b10': 'gear',\n",
    "    'f02': 'lat',\n",
    "    'g04': 'minimum_voltage_battery_subsystem_code',\n",
    "    'g05': 'minimum_voltage_battery_cell_code',\n",
    "    'g06': 'battery_cell_voltage_lowest_value',\n",
    "    'e01': 'engine_status',\n",
    "    'g01': 'maximum_voltage_battery_subsystem_code',\n",
    "    'g02': 'maximum_voltage_battery_cell_code',\n",
    "    'g03': 'battery_cell_voltage_highest_value',\n",
    "    'd07': 'hydrogen_system_highest_temperature_probe_code',\n",
    "    'f03': 'address',\n",
    "    'd06': 'hydrogen_system_highest_temperature',\n",
    "    'f06': 'east_or_west',\n",
    "    'b11': 'can_brake',\n",
    "    'h02': 'battery_faults_count',\n",
    "    'c01': 'motor_count',\n",
    "    'b12': 'can_drive',\n",
    "    'd11': 'hydrogen_maximum_pressure_sensor_code',\n",
    "    'h18': 'soc_jumping_alert',\n",
    "    'h19': 'rechargeable_energy_storage_system_mismatch_alert',\n",
    "    'd05': 'probe_temperature_list',\n",
    "    'd04': 'fuel_cell_temperature_probes_count',\n",
    "    'd03': 'cell_fuel_consumption_rate',\n",
    "    'd02': 'fuel_cell_current',\n",
    "    'd01': 'fuel_cell_voltage',\n",
    "    'd12': 'high_voltage_dc_dc_status',\n",
    "    'h10': 'temperature_difference_alert',\n",
    "    'h11': 'battery_high_temperature_alert',\n",
    "    'h12': 'energy_storage_device_high_voltage_alert',\n",
    "    'h13': 'energy_storage_device_low_voltage_alert',\n",
    "    'h14': 'soc_low_alert',\n",
    "    'h15': 'single_battery_high_voltage_alert',\n",
    "    'd09': 'hydrogen_highest_concentration_sensor_code',\n",
    "    'd08': 'hydrogen_highest_concentration',\n",
    "    'h27': 'motor_temperature_alert',\n",
    "    'h03': 'battery_fault_codes',\n",
    "    'b03': 'running_mode',\n",
    "    'b02': 'charge_status',\n",
    "    'b05': 'mileage',\n",
    "    'b04': 'speed',\n",
    "    'b07': 'current',\n",
    "    'b06': 'voltage',\n",
    "    'b09': 'dc_dc_status',\n",
    "    'b08': 'soc',\n",
    "    'c02': 'motor_info_list',\n",
    "    'g12': 'minimum_temperature_value',\n",
    "    'g11': 'minimum_temperature_probe_code',\n",
    "    'g10': 'minimum_temperature_subsystem_code'}\n",
    "chart_b = {\n",
    "    \"a01\": \"hex_data\",\n",
    "    \"a02\": \"obd_time\",\n",
    "    \"a03\": \"receive_time\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accelerator': '58',\n",
       " 'battery_cell_consistency_low_alert': 'False',\n",
       " 'battery_cell_voltage_highest_value': '7.867',\n",
       " 'battery_cell_voltage_lowest_value': '4.498',\n",
       " 'battery_fault_codes': '[]',\n",
       " 'battery_faults_count': '0',\n",
       " 'battery_high_temperature_alert': 'False',\n",
       " 'brake': '55',\n",
       " 'braking_system_alert': 'False',\n",
       " 'can_brake': 'True',\n",
       " 'can_drive': 'True',\n",
       " 'car_status': '\\\\xE5\\\\x90\\\\xAF\\\\xE5\\\\x8A\\\\xA8',\n",
       " 'cell_fuel_consumption_rate': '1.92',\n",
       " 'charge_status': '\\\\xE5\\\\x85\\\\x85\\\\xE7\\\\x94\\\\xB5\\\\xE5\\\\xAE\\\\x8C\\\\xE6\\\\x88\\\\x90',\n",
       " 'crankshaft_speed': '32681',\n",
       " 'current': '-917.8',\n",
       " 'dc_dc_status': '\\\\xE5\\\\xBC\\\\x82\\\\xE5\\\\xB8\\\\xB8',\n",
       " 'dc_dc_status_alert': 'False',\n",
       " 'dc_dc_temperature_alert': 'False',\n",
       " 'east_or_west': '\\\\xE4\\\\xB8\\\\x9C\\\\xE7\\\\xBB\\\\x8F',\n",
       " 'energy_storage_device_high_voltage_alert': 'False',\n",
       " 'energy_storage_device_low_voltage_alert': 'False',\n",
       " 'energy_storage_device_over_charge_alert': 'False',\n",
       " 'engine_fault_codes': '[]',\n",
       " 'engine_faults_count': '0',\n",
       " 'engine_fuel_consumption_rate': '313.96',\n",
       " 'engine_status': '\\\\xE5\\\\x90\\\\xAF\\\\xE5\\\\x8A\\\\xA8',\n",
       " 'fuel_cell_current': '92.9',\n",
       " 'fuel_cell_temperature_probes_count': '2',\n",
       " 'fuel_cell_voltage': '124.6',\n",
       " 'gear': '6 \\\\xE6\\\\x8C\\\\xA1',\n",
       " 'high_voltage_dc_dc_status': '\\\\xE5\\\\xB7\\\\xA5\\\\xE4\\\\xBD\\\\x9C',\n",
       " 'highest_alert_level': '2 \\\\xE7\\\\xBA\\\\xA7\\\\xE6\\\\x95\\\\x85\\\\xE9\\\\x9A\\\\x9C',\n",
       " 'hvil_alert': 'False',\n",
       " 'hydrogen_highest_concentration': '31015',\n",
       " 'hydrogen_highest_concentration_sensor_code': '108',\n",
       " 'hydrogen_maximum_pressure': '45.9',\n",
       " 'hydrogen_maximum_pressure_sensor_code': '117',\n",
       " 'hydrogen_system_highest_temperature': '-24.0',\n",
       " 'hydrogen_system_highest_temperature_probe_code': '134',\n",
       " 'insulation_alert': 'False',\n",
       " 'insulation_resistance': '16463',\n",
       " 'is_location_valid': 'True',\n",
       " 'lat': '25.528098',\n",
       " 'lng': '99.341907',\n",
       " 'maximum_temperature_probe_code': '113',\n",
       " 'maximum_temperature_subsystem_code': '158',\n",
       " 'maximum_temperature_value': '81',\n",
       " 'maximum_voltage_battery_cell_code': '155',\n",
       " 'maximum_voltage_battery_subsystem_code': '161',\n",
       " 'mileage': '480169400.0',\n",
       " 'minimum_temperature_probe_code': '141',\n",
       " 'minimum_temperature_subsystem_code': '140',\n",
       " 'minimum_temperature_value': '96',\n",
       " 'minimum_voltage_battery_cell_code': '102',\n",
       " 'minimum_voltage_battery_subsystem_code': '133',\n",
       " 'motor_controller_temperature_alert': 'False',\n",
       " 'motor_count': '1',\n",
       " 'motor_fault_codes': '[]',\n",
       " 'motor_faults_count': '0',\n",
       " 'motor_info_list': \"[{'motor_controller_dc_bus_current': -882.4, 'motor_torque': 1234.4, 'motor_controller_temperature': 107, 'motor_status': '\\\\xE5\\\\x87\\\\x86\\\\xE5\\\\xA4\\\\x87\\\\xE7\\\\x8A\\\\xB6\\\\xE6\\\\x80\\\\x81', 'motor_temperature': 72, 'motor_controller_input_voltage': 332.3, 'motor_speed': 16720, 'motor_serial_number': 158}]\",\n",
       " 'motor_temperature_alert': 'False',\n",
       " 'obd_time': '2019-11-11 16:35:13',\n",
       " 'other_fault_codes': '[]',\n",
       " 'other_faults_count': '0',\n",
       " 'probe_temperature_list': '[106, 34]',\n",
       " 'receive_time': '2019-11-11 16:35:13',\n",
       " 'rechargeable_energy_storage_system_mismatch_alert': 'False',\n",
       " 'running_mode': '\\\\xE6\\\\xB7\\\\xB7\\\\xE5\\\\x8A\\\\xA8',\n",
       " 'single_battery_high_voltage_alert': 'False',\n",
       " 'single_battery_low_voltage_alert': 'False',\n",
       " 'soc': '60',\n",
       " 'soc_jumping_alert': 'False',\n",
       " 'soc_low_alert': 'False',\n",
       " 'soc_too_high_alert': 'False',\n",
       " 'south_or_north': '\\\\xE5\\\\x8C\\\\x97\\\\xE7\\\\xBA\\\\xAC',\n",
       " 'speed': '146.4',\n",
       " 'temperature_difference_alert': 'False',\n",
       " 'vin': 'LEWKMH123JF255003',\n",
       " 'voltage': '45.7'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rdd.values().map(lambda row: dict(\n",
    "            (chart.get(json.loads(item).get(\"qualifier\")),  #获取指定键\n",
    "             json.loads(item).get(\"value\")  #获取指定值\n",
    "             .replace('\"', \"temp_data\")\n",
    "             .replace(\"'\", '\"')\n",
    "             .replace(\"temp_data\", \"'\")\n",
    "             ) for item in row.split(\"\\n\")))\n",
    "result.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将rdd转为DF格式并建立临时表\n",
    "#result.saveAsTextFile(\"/User/ppx\")\n",
    "#df=result.toDF().show(1)\n",
    "result.toDF().registerTempTable(\"new_energy_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"psark to hive\") \\\n",
    ".config(\"spark.sql.warehouse.dir\", \"hdfs://cdh1:8020/warehouse/tablespace/external/hive/dfjk.db/\") \\\n",
    ".config(\"hive.metastore.uris\", \"thrif://cdh2:9083\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "|        dfjk|\n",
      "|         sp1|\n",
      "+------------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "|    dfjk|new_energy_obd_info|      false|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()\n",
    "spark.sql(\"use dfjk\").show()\n",
    "#spark.sql(\"create table new_energy_obd_info(accelerator string,battery_cell_consistency_low_alert string,battery_cell_voltage_highest_value string,battery_cell_voltage_lowest_value string,battery_fault_codes string,battery_faults_count string,battery_high_temperature_alert string,brake string,braking_system_alert string,can_brake string,can_drive string,car_status string,cell_fuel_consumption_rate string,charge_status string,crankshaft_speed string,current string,dc_dc_status string,dc_dc_status_alert string,dc_dc_temperature_alert string,east_or_west string,energy_storage_device_high_voltage_alert string,energy_storage_device_low_voltage_alert string,energy_storage_device_over_charge_alert string,engine_fault_codes string,engine_faults_count string,engine_fuel_consumption_rate string,engine_status string,fuel_cell_current string,fuel_cell_temperature_probes_count string,fuel_cell_voltage string,gear string,high_voltage_dc_dc_status string,highest_alert_level string,hvil_alert string,hydrogen_highest_concentration string,hydrogen_highest_concentration_sensor_code string,hydrogen_maximum_pressure string,hydrogen_maximum_pressure_sensor_code string,hydrogen_system_highest_temperature string,hydrogen_system_highest_temperature_probe_code string,insulation_alert string,insulation_resistance string,is_location_valid string,lat string,lng string,maximum_temperature_probe_code string,maximum_temperature_subsystem_code string,maximum_temperature_value string,maximum_voltage_battery_cell_code string,maximum_voltage_battery_subsystem_code string,mileage string,minimum_temperature_probe_code string,minimum_temperature_subsystem_code string,minimum_temperature_value string,minimum_voltage_battery_cell_code string,minimum_voltage_battery_subsystem_code string,motor_controller_temperature_alert string,motor_count string,motor_fault_codes string,motor_faults_count string,motor_info_list string,motor_temperature_alert string,obd_time string,other_fault_codes string,other_faults_count string,probe_temperature_list string,receive_time string,rechargeable_energy_storage_system_mismatch_alert string,running_mode string,single_battery_high_voltage_alert string,single_battery_low_voltage_alert string,soc string,soc_jumping_alert string,soc_low_alert string,soc_too_high_alert string,south_or_north string,speed string,temperature_difference_alert string,vin string,voltage string)location '/warehouse/tablespace/external/hive/dfjk.db/' stored as parquet\") \n",
    "spark.sql(\"show tables\").show()\n",
    "#spark.sql(\"insert into new_energy_obd_info select * from new_energy_temp\")\n",
    "#spark.sql(\"alter table new_energy_obd_info add patition(receive_time)\")\n",
    "#spark.sql(\"alter table new_energy_obd_info add patition(receive_time)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+--------+-------------------+-----------+\n",
      "|database|          tableName|isTemporary|\n",
      "+--------+-------------------+-----------+\n",
      "|    dfjk|new_energy_obd_info|      false|\n",
      "+--------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use dfjk\").show()\n",
    "spark.sql(\"show tables\").show()\n",
    "#spark.sql(\"drop table new_energy_obd_info\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   77830|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o797.csv.\n: java.net.ConnectException: Call From burnishs-iMac-3.local/192.168.3.155 to 192.168.3.155:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1480)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1425)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:93)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1452)\n\t... 49 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-1a3356dbec84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#统计数据条数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select count(*) from new_energy_obd_info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs://192.168.3.155:9000/Users/ppx/Desktop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#spark.sql(\"select * from new_energy_obd_info\").write.saveAsTable(\"new_energy_obd_info1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    929\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ppx/workspace/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o797.csv.\n: java.net.ConnectException: Call From burnishs-iMac-3.local/192.168.3.155 to 192.168.3.155:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:792)\n\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:732)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1480)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)\n\tat sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy21.getFileInfo(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1425)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:93)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:664)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.ConnectException: Connection refused\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\n\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:495)\n\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:615)\n\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:713)\n\tat org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)\n\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1452)\n\t... 49 more\n"
     ]
    }
   ],
   "source": [
    "#19个报警字段\n",
    "df1=spark.sql(\"select temperature_difference_alert,battery_high_temperature_alert,energy_storage_device_high_voltage_alert,energy_storage_device_low_voltage_alert,soc_low_alert,single_battery_high_voltage_alert,single_battery_low_voltage_alert,soc_too_high_alert,soc_jumping_alert,rechargeable_energy_storage_system_mismatch_alert,battery_cell_consistency_low_alert,insulation_alert,dc_dc_temperature_alert,braking_system_alert,dc_dc_status_alert,motor_controller_temperature_alert,hvil_alert,motor_temperature_alert,energy_storage_device_over_charge_alert from new_energy_obd_info\")\n",
    "#统计数据条数\n",
    "spark.sql(\"select count(*) from new_energy_obd_info\").show()\n",
    "#df1.write.csv('/192.168.3.155:8020//Users/ppx/Desktop')\n",
    "#spark.sql(\"select * from new_energy_obd_info\").write.saveAsTable(\"new_energy_obd_info1\")\n",
    "df1.save('hdfs://192.168.3.155:9000/Users/ppx/Desktop')\n",
    "df1.show(n=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-f28813e169b0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-f28813e169b0>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    splited=df1.randomSplit(dict(0.7,0.3),2L)\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import *\n",
    "from pyspark.mllib.classification import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "splited=df1.randomSplit(dict(0.7,0.3),2L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
